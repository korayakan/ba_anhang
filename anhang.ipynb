{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anhang.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/korayakan/ba_anhang/blob/master/anhang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwiw6Y_HWmaM",
        "colab_type": "text"
      },
      "source": [
        "##Installation von benötigten Bibliotheken"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkATudyL8XRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get -qq install tesseract-ocr\n",
        "!pip install -q pytesseract"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UDxP0JS84Oh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q fuzzywuzzy[speedup]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qZxQj-jWy6t",
        "colab_type": "text"
      },
      "source": [
        "##Download des SROI 2019"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fMikmAR88nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "![ -d \"ba_dataset\" ] && echo \"Dataset directory exists.\"\n",
        "![ ! -d \"ba_dataset\" ] \\\n",
        "&& echo \"Dataset directory DOES NOT exist. Cloning from Github...\" \\\n",
        "&& git clone -q https://github.com/korayakan/ba_dataset.git \\\n",
        "&& rm -rf ba_dataset/.git && echo \"Done\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEw650Pk9inr",
        "colab_type": "text"
      },
      "source": [
        "##Funktionen für den Datensatz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiGTSRsZ9DMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for image handling\n",
        "from PIL import Image\n",
        "# for json\n",
        "import json\n",
        "# for glob file search\n",
        "import glob\n",
        "# for fuzzy string comparison\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "# for encoding strings\n",
        "from zlib import crc32\n",
        "# for log configuration\n",
        "import logging\n",
        "# for random numbers\n",
        "from random import randint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCCwkFDL9z5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "COORDINATE_PATH = 'ba_dataset/SROIE2019/0325updated.task1train(626p)'\n",
        "TAG_PATH = 'ba_dataset/SROIE2019/0325updated.task2train(626p)'\n",
        "IMG_PATH = COORDINATE_PATH\n",
        "TAG_TO_IDX = {'': 0, 'company': 1, 'date': 2, 'address': 3, 'total': 4}\n",
        "IDX_TO_TAG = {v: k for k, v in TAG_TO_IDX.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvopvconOCAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_filenames(path, suffix):\n",
        "    path = path + '/' if not path.endswith('/') else path\n",
        "    files = glob.glob(path + '*.' + suffix)\n",
        "    for idx, file in enumerate(files):\n",
        "        files[idx] = file.split(\"/\")[-1].replace('.txt', '').replace('.jpg', '')\n",
        "    return files\n",
        "\n",
        "\n",
        "def get_text_filenames(path):\n",
        "    return get_filenames(path, 'txt')\n",
        "\n",
        "\n",
        "def get_image_filenames(path):\n",
        "    return get_filenames(path, 'jpg')\n",
        "\n",
        "\n",
        "def prepare_data(filename):\n",
        "    tags = read_tags(filename)\n",
        "\n",
        "    im = read_image_file(filename)\n",
        "    width, height = im.size\n",
        "\n",
        "    coordinates = read_normalized_coordinates(filename, width, height)\n",
        "    coordinate_inputs = [coordinates[i][:9] for i in range(len(coordinates))]\n",
        "    coordinate_texts = [coordinates[i][9] for i in range(len(coordinates))]\n",
        "\n",
        "    coordinate_tags = match_coordinate_tags(coordinate_texts, tags)\n",
        "    return tags, coordinate_inputs, coordinate_texts, coordinate_tags\n",
        "\n",
        "\n",
        "def get_all_filenames():\n",
        "    image_files = get_image_filenames(IMG_PATH)\n",
        "    # print('found {} image files'.format(len(image_files)))\n",
        "\n",
        "    coordinate_files = get_text_filenames(COORDINATE_PATH)\n",
        "    # print('found {} files with coordinate data'.format(len(coordinate_files)))\n",
        "\n",
        "    tag_files = get_text_filenames(TAG_PATH)\n",
        "    # print('found {} files with tag data'.format(len(tag_files)))\n",
        "\n",
        "    filenames = list(set(image_files) & set(coordinate_files) & set(tag_files))\n",
        "    filenames.sort()\n",
        "    return filenames\n",
        "\n",
        "\n",
        "def prepare_training_data():\n",
        "    filenames = get_all_filenames()\n",
        "    print('found {} files with coordinate and tag data'.format(len(filenames)))\n",
        "\n",
        "    training_size = int(len(filenames) * 0.8)\n",
        "    print('using {} files for training'.format(training_size))\n",
        "\n",
        "    training_data = []\n",
        "    for i in range(training_size):\n",
        "        tags, coordinate_inputs, coordinate_texts, \\\n",
        "        coordinate_tags = prepare_data(filenames[i])\n",
        "        training_data.append((coordinate_inputs, coordinate_tags))\n",
        "\n",
        "    return training_data\n",
        "\n",
        "\n",
        "def get_random_test_file():\n",
        "    filenames = get_all_filenames()\n",
        "    print('found {} files with coordinate and tag data'.format(len(filenames)))\n",
        "\n",
        "    test_size = int(len(filenames) * 0.2)\n",
        "    print('using {} files for testing'.format(test_size))\n",
        "    \n",
        "    return filenames[randint(test_size + 1, len(filenames) - 1)]\n",
        "\n",
        "\n",
        "def get_test_files():\n",
        "    filenames = get_all_filenames()\n",
        "    print('found {} files with coordinate and tag data'.format(len(filenames)))\n",
        "    \n",
        "    training_size = int(len(filenames) * 0.8)\n",
        "    test_size = int(len(filenames) * 0.2)\n",
        "    print('using {} files for testing'.format(test_size))\n",
        "\n",
        "    test_data = []\n",
        "    for i in range(training_size, len(filenames) - 1):\n",
        "        tags, coordinate_inputs, coordinate_texts, coordinate_tags = \\\n",
        "        prepare_data(filenames[i])\n",
        "        test_data.append(filenames[i])\n",
        "\n",
        "    return test_data\n",
        "\n",
        "\n",
        "def read_text_file(path, filename):\n",
        "    path = path + '/' if not path.endswith('/') else path\n",
        "    with open(path + filename + '.txt') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "\n",
        "def read_text_file_lines(path, filename):\n",
        "    path = path + '/' if not path.endswith('/') else path\n",
        "    lines = []\n",
        "    with open(path + filename + '.txt') as file:\n",
        "        for line in file:\n",
        "            lines.append(line.rstrip('\\n'))\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_coordinates(filename, \\\n",
        "                     path='ba_dataset/SROIE2019/0325updated.task1train(626p)'):\n",
        "    text = read_text_file_lines(path, filename)\n",
        "    coordinates = []\n",
        "    for line in text:\n",
        "        tokens = line.split(',')\n",
        "        line_coordinates = list(map(int, tokens[0:8]))\n",
        "        line_text = ','.join(tokens[8:])\n",
        "        line_coordinates.append(line_text)\n",
        "        coordinates.append(line_coordinates)\n",
        "    return coordinates\n",
        "\n",
        "\n",
        "def read_normalized_coordinates(filename, width, height, \\\n",
        "    path='ba_dataset/SROIE2019/0325updated.task1train(626p)'):\n",
        "    coordinates = read_coordinates(filename, path=path)\n",
        "    for line in coordinates:\n",
        "        for x in range(0, 8, 2):\n",
        "            line[x] /= width\n",
        "        for x in range(1, 8, 2):\n",
        "            line[x] /= height\n",
        "        line.append(line[8])\n",
        "        line[8] = normalize_text(line[8])\n",
        "    return coordinates\n",
        "\n",
        "\n",
        "def read_tags(filename, \\\n",
        "              path='ba_dataset/SROIE2019/0325updated.task2train(626p)'):\n",
        "    return json.loads(read_text_file(path, filename))\n",
        "\n",
        "\n",
        "def read_image_file(filename, \\\n",
        "                    path='ba_dataset/SROIE2019/0325updated.task1train(626p)'):\n",
        "    path = path + '/' if not path.endswith('/') else path\n",
        "    # return cv2.imread(path + filename + '.jpg', 0)\n",
        "    return Image.open(path + filename + '.jpg')\n",
        "\n",
        "\n",
        "def match_coordinate_tags(coordinate_texts, tags):\n",
        "\n",
        "    tags_reverted = {v: k for k, v in tags.items()}\n",
        "    tag_values = list(tags.values())\n",
        "    coordinate_tags = []\n",
        "    for text in coordinate_texts:\n",
        "        tag_guess = process.extractOne(text, tag_values, \\\n",
        "                                       scorer=fuzz.partial_ratio, \\\n",
        "                                       score_cutoff=90)\n",
        "        tag = ''\n",
        "        if tag_guess is not None:\n",
        "            tag = tags_reverted[tag_guess[0]]\n",
        "        coordinate_tags.append(encode_tag(tag))\n",
        "    return coordinate_tags\n",
        "\n",
        "\n",
        "def encode_tag(tag):\n",
        "    return TAG_TO_IDX[tag]\n",
        "\n",
        "\n",
        "def decode_tag(tag_idx):\n",
        "    return IDX_TO_TAG[tag_idx]\n",
        "\n",
        "\n",
        "def normalize_text(input_text, encoding=\"utf-8\"):\n",
        "    return float(crc32(input_text.encode(encoding)) & 0xffffffff) / 2**32\n",
        "\n",
        "\n",
        "def combine_predicted_tags(texts, tags):\n",
        "    company = ''\n",
        "    date = ''\n",
        "    address = ''\n",
        "    total = ''\n",
        "\n",
        "    for i in range(len(tags)):\n",
        "        if tags[i] == 1 and not company.endswith(texts[i]):\n",
        "            company += ' ' + texts[i]\n",
        "        if tags[i] == 2 and not date.endswith(texts[i]):\n",
        "            date += ' ' + texts[i]\n",
        "        if tags[i] == 3 and not address.endswith(texts[i]):\n",
        "            address += ' ' + texts[i]\n",
        "        if tags[i] == 4 and not total.endswith(texts[i]):\n",
        "            total += ' ' + texts[i]\n",
        "\n",
        "    return {'company': company.strip(), 'date': date.strip(), \\\n",
        "            'address': address.strip(), 'total': total.strip()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVHqX5Ob-QHS",
        "colab_type": "text"
      },
      "source": [
        "##Definition des LSTM Netzes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNxb2_d6-Gcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as opt\n",
        "import torch.nn.functional as fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzE1aLZ--ayK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SERIALIZED_MODEL_NAME = 'ba_model.pt'\n",
        "\n",
        "INPUT_SIZE = 9\n",
        "OUTPUT_SIZE = 5\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size=6, num_of_layers=1):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # The LSTM takes coordinates as input, and outputs hidden states\n",
        "        self.lstm = nn.LSTM(INPUT_SIZE, hidden_size, num_layers=num_of_layers)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_size, OUTPUT_SIZE)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        hidden_space, _ = self.lstm(input_seq)\n",
        "        tag_space = self.hidden2tag(hidden_space.view(len(input_seq), -1))\n",
        "        tag_scores = fn.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "\n",
        "\n",
        "def load_model(model_path=SERIALIZED_MODEL_NAME):\n",
        "    if os.path.isfile(model_path):\n",
        "        model = torch.load(model_path)\n",
        "        model.eval()\n",
        "        return model\n",
        "    else:\n",
        "        print('Model was not trained yet!')\n",
        "        sys.exit(0)\n",
        "\n",
        "\n",
        "def evaluate(coordinate_inputs, model_path=SERIALIZED_MODEL_NAME):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # print('Using {} for prediction'.format(device))\n",
        "    with torch.no_grad():\n",
        "        model = load_model(model_path)\n",
        "        model.to(device)\n",
        "        input_seq = torch.tensor(coordinate_inputs)\n",
        "        input_seq = input_seq.unsqueeze(1)\n",
        "        input_seq = input_seq.to(device)\n",
        "        tag_scores = model(input_seq)\n",
        "        probabilities, tags = tag_scores.topk(1)\n",
        "        predictions = []\n",
        "        for i in range(len(probabilities)):\n",
        "            predictions.append((torch.exp(probabilities[i]).item(), \\\n",
        "                                tags[i].item()))\n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYALmQrV-rdx",
        "colab_type": "text"
      },
      "source": [
        "##Funktionen für das Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knf3Okr_-gHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, print_every=1, learning_rate=0.1, hidden_size=6, \\\n",
        "          num_of_layers=1):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print('Using {} for training'.format(device))\n",
        "\n",
        "    model = LSTM(hidden_size=hidden_size, num_of_layers=num_of_layers)\n",
        "    loss_function = nn.NLLLoss()\n",
        "    optimizer = opt.SGD(model.parameters(), lr=learning_rate)\n",
        "    model.to(device)\n",
        "\n",
        "    data_set = prepare_training_data()\n",
        "    split_data = {0: [], 1: [], 2: [], 3: []}\n",
        "    for i in range(len(data_set)):\n",
        "        if i % 4 == 0:\n",
        "            split_data[0].append(data_set[i])\n",
        "        if i % 4 == 1:\n",
        "            split_data[1].append(data_set[i])\n",
        "        if i % 4 == 2:\n",
        "            split_data[2].append(data_set[i])\n",
        "        if i % 4 == 3:\n",
        "            split_data[3].append(data_set[i])\n",
        "    # for i in range(4):\n",
        "    #     print(len(split_data[i]))\n",
        "\n",
        "    steps = 0\n",
        "    running_loss = 0\n",
        "    train_losses, test_losses = [], []\n",
        "    for epoch in range(epochs):\n",
        "        steps += 1\n",
        "        validation_idx = epoch % 4\n",
        "        validation_data = split_data[validation_idx]\n",
        "        training_data = []\n",
        "        for i in range(4):\n",
        "            if i != validation_idx:\n",
        "                training_data.extend(split_data[i])\n",
        "\n",
        "        for coordinate_inputs, coordinate_tags in training_data:\n",
        "            # print(coordinate_inputs)\n",
        "            # print(coordinate_tags)\n",
        "\n",
        "            # Step 1. Clear Pytorch gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Step 2. Get inputs ready for the network\n",
        "            input_seq = torch.tensor(coordinate_inputs)\n",
        "            input_seq = input_seq.unsqueeze(1)\n",
        "            input_seq = input_seq.to(device)\n",
        "            targets = torch.tensor(coordinate_tags)\n",
        "            targets = targets.to(device)\n",
        "            # print(input_seq)\n",
        "            # print(targets)\n",
        "\n",
        "            # Step 3. Run forward pass\n",
        "            tag_scores = model(input_seq)\n",
        "\n",
        "            # Step 4. Compute the loss, gradients, and update the parameters\n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            # print(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if (steps == 1) or (steps % print_every == 0):\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for coordinate_inputs_val, coordinate_tags_val \\\n",
        "                in validation_data:\n",
        "                    input_seq_val = torch.tensor(coordinate_inputs_val)\n",
        "                    input_seq_val = input_seq_val.unsqueeze(1)\n",
        "                    input_seq_val = input_seq_val.to(device)\n",
        "                    targets_val = torch.tensor(coordinate_tags_val)\n",
        "                    targets_val = targets_val.to(device)\n",
        "\n",
        "                    tag_scores_val = model.forward(input_seq_val)\n",
        "                    loss_val = loss_function(tag_scores_val, targets_val)\n",
        "                    test_loss += loss_val.item()\n",
        "\n",
        "            train_losses.append(running_loss / (len(training_data) \\\n",
        "                                                * print_every))\n",
        "            test_losses.append(test_loss / len(validation_data))\n",
        "            print_trainloss=running_loss / (len(training_data) * print_every)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}.. \"\n",
        "                  f\"Train loss: {print_trainloss:.3f}.. \"\n",
        "                  f\"Test loss: {test_loss / len(validation_data):.3f}.. \")\n",
        "                  #f\"Test accuracy: {accuracy / len(testloader):.3f}\")\n",
        "            running_loss = 0\n",
        "            model.train()\n",
        "\n",
        "    torch.save(model, SERIALIZED_MODEL_NAME)\n",
        "    return train_losses, test_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7o3MzEp--TU",
        "colab_type": "text"
      },
      "source": [
        "##Funktionen für die Vorhersage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tljDdYzD_AH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(filename, model_path=SERIALIZED_MODEL_NAME):\n",
        "    print('File:')\n",
        "    print(filename, '\\n')\n",
        "\n",
        "    tags, coordinate_inputs, coordinate_texts, coordinate_tags = \\\n",
        "    prepare_data(filename)\n",
        "\n",
        "    print('Expected category tags:')\n",
        "    print(tags, '\\n')\n",
        "\n",
        "    # print('Input coordinates:')\n",
        "    # print(coordinate_inputs)\n",
        "    print(coordinate_inputs)\n",
        "    print(coordinate_texts)\n",
        "    print('Expected coordinate tags:')\n",
        "    print(coordinate_tags)\n",
        "\n",
        "    predictions = evaluate(coordinate_inputs, model_path)\n",
        "    predicted_coordinate_tags = []\n",
        "    for prediction in predictions:\n",
        "        predicted_coordinate_tags.append(prediction[1])\n",
        "    probabilities = []\n",
        "    for prediction in predictions:\n",
        "        probabilities.append(\"{:.0%}\".format(prediction[0]))\n",
        "    print('Predicted coordinate tags:')\n",
        "    print(predicted_coordinate_tags)\n",
        "    print('Confidence:')\n",
        "    print(probabilities, '\\n')\n",
        "\n",
        "    predicted_tags = combine_predicted_tags(coordinate_texts, \\\n",
        "                                            predicted_coordinate_tags)\n",
        "    print(predicted_tags)\n",
        "\n",
        "\n",
        "def get_expected_tags(filename):\n",
        "    tags, coordinate_inputs, coordinate_texts, coordinate_tags = \\\n",
        "    prepare_data(filename)\n",
        "    return coordinate_tags\n",
        "\n",
        "\n",
        "def get_predicted_tags(filename, model_path=SERIALIZED_MODEL_NAME):\n",
        "    tags, coordinate_inputs, coordinate_texts, coordinate_tags = \\\n",
        "    prepare_data(filename)\n",
        "    predictions = evaluate(coordinate_inputs, model_path)\n",
        "    predicted_coordinate_tags = []\n",
        "    for prediction in predictions:\n",
        "        predicted_coordinate_tags.append(prediction[1])\n",
        "    return predicted_coordinate_tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw8GLZWI_XLN",
        "colab_type": "text"
      },
      "source": [
        "##OCR-Modul"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z7oJi9J_ZmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = read_image_file('X00016469612')\n",
        "img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh5t6Gn5_mhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytesseract\n",
        "print(pytesseract.image_to_string(img, lang='eng', config='--oem 1'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TtsGx1bXgpJ",
        "colab_type": "text"
      },
      "source": [
        "Der folgende Funktionsaufruf gibt für jedes Wort Details inklusive des Konfidenzlevels und der Koordinaten aus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh0JJozi_9G0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(pytesseract.image_to_data(img, lang='eng', config='--oem 1'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb1PEwceACx3",
        "colab_type": "text"
      },
      "source": [
        "##KI-Modul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-elI3I6YCyc",
        "colab_type": "text"
      },
      "source": [
        "###Training des KI-Moduls\n",
        "Als Hyperparameter werden verwendet:\n",
        "\n",
        "\n",
        "*   Epoch: 1000\n",
        "*   Lernrate: 0,05\n",
        "* Anzahl der Knoten pro Schicht: 256\n",
        "* Anzahl der Schichten: 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOj1-ZW2APga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime, timezone\n",
        "import time\n",
        "start = time.time()\n",
        "now = datetime.now(timezone.utc).astimezone()\n",
        "print('start model training at {}'.format(now))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xwP78t5AdSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_losses, test_losses = train(1000, print_every = 100, \\\n",
        "                                   learning_rate = 0.05, hidden_size=256, \\\n",
        "                                   num_of_layers=3)\n",
        "#train_losses, test_losses = train(10, print_every = 1, \\\n",
        "#                                 learning_rate = 0.05, hidden_size=256, \\\n",
        "#                                  num_of_layers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCCGAIKvLM1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "now = datetime.now(timezone.utc).astimezone()\n",
        "done = time.time()\n",
        "elapsed = int(round(done - start))\n",
        "print('training finished at {}, duration was {} min {} sec'\\\n",
        "      .format(now, int(round(elapsed / 60)), elapsed % 60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytKDZGNtY2qr",
        "colab_type": "text"
      },
      "source": [
        "###Schaubild des Lernverlaufs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mPi-vvKLSam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(test_losses, label='Validation loss')\n",
        "plt.legend(frameon=False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_QoK0pfZERJ",
        "colab_type": "text"
      },
      "source": [
        "Das gespeicherte Model kann für Vorhersagen verwendet werden"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4vE1slMLViz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predict(get_random_test_file())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLeCeX9pY9wt",
        "colab_type": "text"
      },
      "source": [
        "##Evaluierung des KI-Moduls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_BvqXb2NP7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GI_F-y7NSgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "expected = []\n",
        "predicted = []\n",
        "for filename in get_test_files():\n",
        "  #print('File: ' + filename)\n",
        "  expected.extend(get_expected_tags(filename))\n",
        "  predicted.extend(get_predicted_tags(filename, 'ba_model.pt'))\n",
        "\n",
        "print()\n",
        "print(\"Konfusionsmatrix\")\n",
        "print(metrics.confusion_matrix(expected, predicted))\n",
        "print()\n",
        "print(\"Evaluierungskennzahlen\")\n",
        "print(metrics.classification_report(expected, predicted, digits=3))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}